# ==================== –£–õ–£–ß–®–ï–ù–ù–´–ô –ö–õ–ê–°–° –ò–ò –ú–û–î–ï–õ–ò ====================
import os
import tempfile
import traceback
from datetime import datetime
from urllib.request import urlretrieve

import joblib
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import BayesianRidge
from sklearn.preprocessing import StandardScaler

import library as l


class EnhancedStockAIModel:
    def __init__(self, model_path='enhanced_stock_ai_model.joblib'):
        self.model_path = model_path
        self.model = None
        self.probabilistic_model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        self.training_history = []
        self.model_metadata = {}

        # –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        self._initialize_model_with_fallback()

    def _initialize_model_with_fallback(self):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å —Å–∏—Å—Ç–µ–º–æ–π fallback"""
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò –º–æ–¥–µ–ª–µ–π...")

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
        if self._try_load_existing_model():
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
            return

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        if self._try_download_pretrained_model():
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
            return

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        print("üìù –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self._create_new_models()
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å")

    def _try_load_existing_model(self):
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏"""
        try:
            if not os.path.exists(self.model_path):
                print("üìù –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return False

            print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}")
            model_data = joblib.load(self.model_path)

            if isinstance(model_data, dict):
                return self._load_from_dict(model_data)
            else:
                return self._load_old_format(model_data)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

    def _try_download_pretrained_model(self):
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            # –í —Ä–µ–∞–ª—å–Ω–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ —ç—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –º–æ–¥–µ–ª–∏ —Å HuggingFace –∏–ª–∏ –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            pretrained_url = l.PRETRAINED_MODEL_URLS.get('base_model')

            if not pretrained_url:
                print("‚ÑπÔ∏è URL –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ —É–∫–∞–∑–∞–Ω—ã")
                return False

            print(f"üåê –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            with tempfile.NamedTemporaryFile(delete=False, suffix='.joblib') as tmp_file:
                temp_path = tmp_file.name

            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                urlretrieve(pretrained_url, temp_path)

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
                model_data = joblib.load(temp_path)

                if isinstance(model_data, dict):
                    success = self._load_from_dict(model_data)
                    if success:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                        self._save_models()
                        print("üíæ –ü—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ")
                    return success

            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

            return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False

    def _load_from_dict(self, model_data):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        try:
            required_keys = ['main_model', 'probabilistic_model', 'scaler']
            if not all(key in model_data for key in required_keys):
                print("‚ùå –í —Ñ–∞–π–ª–µ –º–æ–¥–µ–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã")
                return False

            self.model = model_data['main_model']
            self.probabilistic_model = model_data['probabilistic_model']
            self.scaler = model_data['scaler']
            self.is_trained = model_data.get('is_trained', True)
            self.training_history = model_data.get('training_history', [])
            self.model_metadata = model_data.get('model_metadata', {})

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
            if self.model is None or self.probabilistic_model is None:
                print("‚ùå –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞–≤–Ω—ã None")
                return False

            print(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.training_history)} —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫, "
                  f"{self.model_metadata.get('total_training_samples', 0)} samples")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ —Å–ª–æ–≤–∞—Ä—è: {e}")
            return False

    def _load_old_format(self, model_data):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –º–æ–¥–µ–ª–∏"""
        try:
            if isinstance(model_data, (list, tuple)) and len(model_data) >= 2:
                self.model = model_data[0]
                self.probabilistic_model = model_data[1]
                self.is_trained = True
                return True
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞: {e}")
            return False

    def _create_new_models(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        try:
            # RandomForest —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º warm_start –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
            self.model = RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                warm_start=True  # –í–∞–∂–Ω–æ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è!
            )

            # BayesianRidge –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
            self.probabilistic_model = BayesianRidge(
                n_iter=200,
                tol=1e-3
            )

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º scaler
            self.scaler = StandardScaler()

            self.is_trained = False
            self.model_metadata = {
                'creation_date': datetime.now().isoformat(),
                'total_training_samples': 0,
                'training_sessions': 0,
                'last_training': None,
                'model_type': 'EnhancedStockAIModel_v5',
                'supports_incremental_learning': True
            }

            return True

        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–µ–π—à–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            self.model = RandomForestRegressor(n_estimators=10, random_state=42)
            self.probabilistic_model = BayesianRidge()
            self.is_trained = False
            return False

    def train_models(self, prices_df, news_sentiment_by_date, incremental=True):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
        try:
            print("üîÑ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π
            if self.model is None or self.probabilistic_model is None:
                print("‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ...")
                self._create_new_models()

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X, y = self.prepare_training_data(prices_df, news_sentiment_by_date)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
            if not self._validate_training_data(X, y):
                print("‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return False

            print(f"üéØ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(X)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –¥–æ–æ–±—É—á–µ–Ω–∏—è
            try:
                if self.is_trained and incremental and hasattr(self.scaler, 'n_features_in_'):
                    # –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è - partial_fit –∏–ª–∏ transform
                    if hasattr(self.scaler, 'partial_fit'):
                        self.scaler.partial_fit(X)
                    X_processed = self.scaler.transform(X)
                else:
                    # –ü–µ—Ä–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –ø–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
                    X_processed = self.scaler.fit_transform(X)
            except Exception as scale_error:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {scale_error}, –ø—Ä–∏–º–µ–Ω—è–µ–º fit_transform")
                X_processed = self.scaler.fit_transform(X)

            # –û–ë–£–ß–ï–ù–ò–ï –° –ü–û–î–î–ï–†–ñ–ö–û–ô –î–û–û–ë–£–ß–ï–ù–ò–Ø
            if self.is_trained and incremental:
                print("üìö –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏...")
                # –î–ª—è RandomForest —Å warm_start=True –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å n_estimators
                if hasattr(self.model, 'warm_start') and self.model.warm_start:
                    current_estimators = self.model.n_estimators
                    self.model.n_estimators = current_estimators + 50
                    self.model.fit(X_processed, y)
                else:
                    self.model.fit(X_processed, y)

                # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ
                self.probabilistic_model.fit(X_processed, y)
            else:
                # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                print("üéì –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
                self.model.fit(X_processed, y)
                self.probabilistic_model.fit(X_processed, y)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.is_trained = True
            training_type = "incremental" if (self.is_trained and incremental) else "full"
            self._update_training_metadata(len(X), training_type)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            self._save_models()

            print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            traceback.print_exc()
            return False

    def _update_training_metadata(self, new_samples_count, training_type):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        current_total = self.model_metadata.get('total_training_samples', 0)
        self.model_metadata['total_training_samples'] = current_total + new_samples_count
        self.model_metadata['training_sessions'] = self.model_metadata.get('training_sessions', 0) + 1
        self.model_metadata['last_training'] = datetime.now().isoformat()
        self.model_metadata['last_training_type'] = training_type

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫
        self.training_history.append({
            'timestamp': datetime.now().isoformat(),
            'samples': new_samples_count,
            'total_samples': self.model_metadata['total_training_samples'],
            'type': training_type
        })

    @staticmethod
    def _validate_training_data(X, y):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if X is None or y is None:
            print("‚ùå –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞–≤–Ω—ã None")
            return False

        if len(X) == 0 or len(y) == 0:
            print("‚ùå –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False

        if len(X) != len(y):
            print("‚ùå –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ X –∏ y")
            return False

        return True

    def _save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        try:
            if self.model is None or self.probabilistic_model is None:
                print("‚ö†Ô∏è –ù–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å: –º–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                return False

            models_to_save = {
                'main_model': self.model,
                'probabilistic_model': self.probabilistic_model,
                'scaler': self.scaler,
                'is_trained': self.is_trained,
                'training_history': self.training_history,
                'model_metadata': self.model_metadata
            }

            joblib.dump(models_to_save, self.model_path)
            print(f"üíæ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.model_path}")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    def create_advanced_features(self, prices_df, news_sentiment):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = {}

        if prices_df.empty:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return features

        for ticker in prices_df.columns:
            try:
                if ticker not in prices_df:
                    continue

                price_series = prices_df[ticker].dropna()
                if len(price_series) < 30:
                    continue

                # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                returns = price_series.pct_change().dropna()
                if len(returns) < 15:
                    continue

                # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                volatility = returns.std()
                current_price = price_series.iloc[-1]
                ma_short = price_series.rolling(window=5).mean().iloc[-1]
                ma_long = price_series.rolling(window=20).mean().iloc[-1]

                features[ticker] = {
                    'volatility': float(volatility) if not pd.isna(volatility) else 0.02,
                    'price_ratio': float(current_price / ma_long) if ma_long != 0 else 1.0,
                    'ma_ratio': float(ma_short / ma_long) if ma_long != 0 else 1.0,
                    'sector_sentiment': float(news_sentiment.get(self.get_sector_for_ticker(ticker), 0))
                }

            except Exception as e:
                print(e)  # TODO: define
                continue

        return features

    @staticmethod
    def get_sector_for_ticker(ticker):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–∏–∫–µ—Ä–∞"""
        for sector, tickers in l.RUSSIAN_SECTORS.items():
            if ticker in tickers:
                return sector
        return '–û–±—â–∏–µ'

    def prepare_training_data(self, prices_df, news_sentiment_by_date, forecast_days=5):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        X, y = [], []

        if prices_df.empty:
            return np.array([]), np.array([])

        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        current_sentiment = {}
        if news_sentiment_by_date:
            latest_date = max(news_sentiment_by_date.keys())
            current_sentiment = news_sentiment_by_date[latest_date]

        features_dict = self.create_advanced_features(prices_df, current_sentiment)

        for ticker, features in features_dict.items():
            if features and len(features) > 0:
                feature_vector = list(features.values())
                # –ü—Ä–æ—Å—Ç–∞—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                target = np.random.uniform(-0.1, 0.1)  # –ó–∞–º–µ–Ω–∏—Ç–µ —Ä–µ–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
                X.append(feature_vector)
                y.append(target)

        if len(X) > 0:
            return np.array(X), np.array(y)
        else:
            return np.array([]), np.array([])

    def predict_with_confidence(self, current_features):
        """–ü—Ä–æ–≥–Ω–æ–∑ —Å –æ—Ü–µ–Ω–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        if not self.is_trained or self.model is None:
            return 0.0, 0.0

        try:
            if isinstance(current_features, dict):
                feature_vector = list(current_features.values())
            else:
                feature_vector = current_features

            if len(feature_vector) == 0:
                return 0.0, 0.0

            features_array = np.array(feature_vector).reshape(1, -1)
            features_array = self.scaler.transform(features_array)

            main_pred = self.model.predict(features_array)[0]
            probabilistic_pred, probabilistic_std = self.probabilistic_model.predict(features_array, return_std=True)

            combined_pred = 0.7 * main_pred + 0.3 * probabilistic_pred
            confidence = max(0.0, min(1.0, 1.0 - probabilistic_std[0]))

            return float(combined_pred), float(confidence)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return 0.0, 0.0

    def get_training_info(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ –º–æ–¥–µ–ª–∏"""
        if not self.training_history:
            return "–ú–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å"

        latest = self.training_history[-1]
        return {
            'last_training': latest['timestamp'],
            'samples': latest['total_samples'],
            'total_trainings': len(self.training_history),
            'model_metadata': self.model_metadata
        }

    def get_model_status(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–¥–µ–ª–∏"""
        return {
            'is_trained': self.is_trained,
            'model_initialized': self.model is not None,
            'training_sessions': len(self.training_history),
            'total_samples': self.model_metadata.get('total_training_samples', 0),
            'supports_incremental': self.model_metadata.get('supports_incremental_learning', False)
        }