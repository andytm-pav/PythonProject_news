import os
import sys
import time
import requests
import feedparser
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from tqdm import tqdm
import warnings
import joblib
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import re

warnings.filterwarnings('ignore')

# ---------------------------
# 1) –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –†–û–°–°–ò–ô–°–ö–ò–• –°–ï–ö–¢–û–†–û–í –ò –ê–ö–¶–ò–ô
# ---------------------------

RUSSIAN_SECTORS = {
    '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞': ['GAZP', 'LKOH', 'ROSN', 'SNGS', 'TATN'],
    '–§–∏–Ω–∞–Ω—Å—ã': ['SBER', 'VTBR', 'MOEX', 'TCSG'],
    '–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è': ['GMKN', 'NLMK', 'MAGN', 'PLZL', 'RUAL'],
    '–¢–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏': ['MTSS', 'RTKM', 'MGNT'],
    '–•–∏–º–∏—è': ['PHOR', 'AKRN', 'ODVA'],
    '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç': ['AFLT', 'NMTP', 'TRMK'],
    '–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ': ['PIKK', 'LSRG', 'UNAC'],
    'IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': ['YNDX', 'OZON', 'VKCO'],
    '–†–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è': ['FIVE', 'DSKY', 'LNTA'],
    '–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞': ['POLY', 'RGSS'],
    '–≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞': ['FEES', 'HYDR', 'IRAO']
}

SECTOR_MAPPING = {
    '–Ω–µ—Ñ—Ç—å': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', '–≥–∞–∑': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', '–Ω–µ—Ñ—Ç–µ–≥–∞–∑': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', '—ç–Ω–µ—Ä–≥': '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞',
    '–±–∞–Ω–∫': '–§–∏–Ω–∞–Ω—Å—ã', '—Ñ–∏–Ω–∞–Ω—Å': '–§–∏–Ω–∞–Ω—Å—ã', '—Å–±–µ—Ä': '–§–∏–Ω–∞–Ω—Å—ã', '–≤—Ç–±': '–§–∏–Ω–∞–Ω—Å—ã', '–±–∏—Ä–∂–∞': '–§–∏–Ω–∞–Ω—Å—ã',
    '–º–µ—Ç–∞–ª–ª': '–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è', '—Å—Ç–∞–ª—å': '–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è', '–Ω–∏–∫–µ–ª—å': '–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è', '–∞–ª—é–º–∏–Ω': '–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è',
    '—Å–≤—è–∑—å': '–¢–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏', '—Ç–µ–ª–µ–∫–æ–º': '–¢–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏', '–º—Ç—Å': '–¢–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏',
    '—Ö–∏–º–∏—è': '–•–∏–º–∏—è', '—É–¥–æ–±—Ä–µ–Ω': '–•–∏–º–∏—è', '–Ω–µ—Ñ—Ç–µ—Ö–∏–º': '–•–∏–º–∏—è',
    '–∞–≤–∏–∞': '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç': '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–ø–æ—Ä—Ç': '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–∞—ç—Ä–æ–ø–æ—Ä—Ç': '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç',
    '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å': '–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–¥–µ–≤–µ–ª–æ–ø–µ—Ä': '–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–Ω–µ–¥–≤–∏–∂': '–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
    '–∏—Ç': 'IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '—Ç–µ—Ö–Ω–æ–ª–æ–≥': 'IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç': 'IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', '—Å–æ—Ñ—Ç': 'IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏',
    '—Ä–∏—Ç–µ–π–ª': '–†–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è', '–º–∞–≥–∞–∑–∏–Ω': '–†–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è', '—Ç–æ—Ä–≥–æ–≤–ª—è': '–†–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è',
    '—Ñ–∞—Ä–º–∞': '–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞', '–º–µ–¥–∏—Ü–∏–Ω': '–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞', '–ª–µ–∫–∞—Ä—Å—Ç–≤': '–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞',
    '—ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥': '–≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', '—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç': '–≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞'
}

# ---------------------------
# 2) RSS –õ–ï–ù–¢–´ –†–û–°–°–ò–ô–°–ö–ò–• –§–ò–ù–ê–ù–°–û–í–´–• –ù–û–í–û–°–¢–ï–ô
# ---------------------------

RUSSIAN_FEEDS = {
    "–í–µ–¥–æ–º–æ—Å—Ç–∏": "https://www.vedomosti.ru/rss/news",
    "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç": "https://www.kommersant.ru/RSS/news.xml",
    "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å": "https://www.interfax.ru/rss.asp",
    "–¢–ê–°–°": "https://tass.ru/rss/v2.xml",
    "–õ–µ–Ω—Ç–∞.—Ä—É": "https://lenta.ru/rss/news",
    "–§–∏–Ω–∞–º": "https://www.finam.ru/analysis/conews/rsspoint/",
    "–ë–§–ú": "https://www.bfm.ru/news.rss?rubric=19",
    "–ö–æ–º–µ—Ä—Å–∞–Ω—Ç": "https://www.kommersant.ru/rss/news.xml",
    "–ê–ª—å—Ç–∞_–∑–∞–∫–æ–Ω": "http://www.alta.ru/rss/laws_news/",
    "—Ñ–∏–Ω–∞–º –º–∏—Ä–æ–≤—ã—Ö": "https://www.finam.ru/international/advanced/rsspoint/",
    "–ò–Ω–≤–∏—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º": "https://ru.investing.com/rss/news_356.rss",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º —Å—ã—Ä—å—ë": "https://ru.investing.com/rss/news_11.rss",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º –≠–∫–æ–Ω–æ–º–∏–∫–∏": "https://ru.investing.com/rss/news_14.rss",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º —Ñ–æ–Ω–¥–æ–≤—ã–π": "https://ru.investing.com/rss/news_25.rss",
}


# ---------------------------
# 3) –ö–õ–ê–°–° –î–õ–Ø –ü–û–õ–£–ß–ï–ù–ò–Ø –î–ê–ù–ù–´–• –û –†–û–°–°–ò–ô–°–ö–ò–• –ê–ö–¶–ò–Ø–•
# ---------------------------

class RussianStockData:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∞–∫—Ü–∏—è—Ö"""

    @staticmethod
    def get_moex_data_safe(tickers, days=30):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –±–∏—Ä–∂–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        base_url = "https://iss.moex.com/iss/engines/stock/markets/shares/boards/TQBR/securities/{}/candles.json"

        prices_data = {}
        successful_downloads = 0

        for ticker in tqdm(tickers, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MOEX"):
            try:
                url = base_url.format(ticker)
                from_date = (datetime.now() - timedelta(days=days * 2)).strftime('%Y-%m-%d')
                till_date = datetime.now().strftime('%Y-%m-%d')

                response = requests.get(url, params={
                    'from': from_date,
                    'till': till_date,
                    'interval': 24
                }, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    candles = data.get('candles', {}).get('data', [])

                    if candles:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã –∏ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è
                        dates = []
                        closes = []

                        for candle in candles:
                            date_str = candle[0]  # –î–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ '2024-01-15'
                            close_price = candle[1]  # –¶–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è

                            if date_str and close_price:
                                dates.append(date_str)
                                closes.append(close_price)

                        if dates and closes:
                            # –°–æ–∑–¥–∞–µ–º Series –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞
                            series = pd.Series(closes, index=dates)

                            # –£–î–ê–õ–Ø–ï–ú –î–£–ë–õ–ò–ö–ê–¢–´ –ò–ù–î–ï–ö–°–ê (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ)
                            if not series.index.is_unique:
                                print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {ticker}. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã...")
                                series = series[~series.index.duplicated(keep='first')]

                            prices_data[ticker] = series
                            successful_downloads += 1

                time.sleep(1)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤ API

            except requests.exceptions.Timeout:
                print(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {ticker}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {ticker}: {e}")
                continue

        print(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {successful_downloads} —Ç–∏–∫–µ—Ä–æ–≤")
        return prices_data

    @staticmethod
    def create_dataframe_from_series(series_dict, min_length=10):
        """–°–æ–∑–¥–∞–Ω–∏–µ DataFrame –∏–∑ —Å–ª–æ–≤–∞—Ä—è Series —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        if not series_dict:
            return pd.DataFrame()

        # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö Series
        max_length = max(len(series) for series in series_dict.values())

        if max_length < min_length:
            print(f"–î–∞–Ω–Ω—ã–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ (max_length={max_length}). –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.")
            return pd.DataFrame()

        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º
        prices_df = pd.DataFrame()

        for ticker, series in series_dict.items():
            if len(series) == max_length:
                # –ï—Å–ª–∏ –¥–ª–∏–Ω–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                prices_df[ticker] = series
            else:
                # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—É, –∑–∞–ø–æ–ª–Ω—è—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è NaN
                aligned_prices = pd.Series([np.nan] * max_length)

                # –ö–æ–ø–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                if len(series) > 0:
                    aligned_prices.iloc[:len(series)] = series.values

                prices_df[ticker] = aligned_prices

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —É–Ω–∏–∫–∞–ª–µ–Ω
        if not prices_df.index.is_unique:
            print("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º DataFrame. –£–¥–∞–ª—è–µ–º...")
            prices_df = prices_df[~prices_df.index.duplicated(keep='first')]

        return prices_df

    @staticmethod
    def create_realistic_test_prices(tickers, days=30):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∞–∫—Ü–∏–π"""
        print("–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        # –°–æ–∑–¥–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
        dates = pd.date_range(end=datetime.now(), periods=days, freq='D')

        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω—ã –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∞–∫—Ü–∏–π (–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–∞ 2024 –≥–æ–¥)
        base_prices = {
            'SBER': 250, 'GAZP': 160, 'LKOH': 5800, 'GMKN': 25000, 'NLMK': 180,
            'MTSS': 270, 'ROSN': 420, 'TATN': 380, 'VTBR': 0.03, 'MOEX': 150,
            'MAGN': 45000, 'PLZL': 12000, 'PHOR': 8000, 'AKRN': 6500, 'AFLT': 40,
            'PIKK': 1100, 'YNDX': 2900, 'OZON': 2200, 'FIVE': 2100, 'POLY': 700,
            'SNGS': 40, 'TCSG': 3000, 'RUAL': 300, 'ODVA': 200, 'TRMK': 1500,
            'LSRG': 800, 'UNAC': 500, 'VKCO': 400, 'DSKY': 600, 'LNTA': 2000,
            'RGSS': 300, 'FEES': 20, 'HYDR': 1, 'IRAO': 3
        }

        prices_df = pd.DataFrame(index=dates)

        for ticker in tickers:
            if ticker in base_prices:
                base_price = base_prices[ticker]
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è —Ü–µ–Ω —Å —Ç—Ä–µ–Ω–¥–æ–º
                np.random.seed(hash(ticker) % 10000)  # –†–∞–∑–Ω—ã–µ —Å–∏–¥—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤
                returns = np.random.normal(0.0005, 0.02, days)  # –ù–µ–±–æ–ª—å—à–æ–π –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π —Ç—Ä–µ–Ω–¥

                prices = [base_price]
                for ret in returns[1:]:
                    new_price = prices[-1] * (1 + ret)
                    prices.append(max(new_price, 0.01))  # –ó–∞—â–∏—Ç–∞ –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–Ω

                prices_df[ticker] = prices
            else:
                # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                base_price = np.random.uniform(50, 1000)
                returns = np.random.normal(0, 0.015, days)

                prices = [base_price]
                for ret in returns[1:]:
                    new_price = prices[-1] * (1 + ret)
                    prices.append(max(new_price, 0.01))

                prices_df[ticker] = prices

        return prices_df


# ---------------------------
# 4) –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–ë–û–†–ê –ù–û–í–û–°–¢–ï–ô
# ---------------------------

def fetch_russian_feeds_safe(feeds_dict, max_items_per_feed=10):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å RSS-–ª–µ–Ω—Ç —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    rows = []

    for name, url in tqdm(feeds_dict.items(), desc="–°–±–æ—Ä —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"):
        try:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/rss+xml, application/xml, text/xml'
            }

            feed = feedparser.parse(url)
            time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è {name}: {e}")
            continue

        if not hasattr(feed, 'entries') or not feed.entries:
            print(f"–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {name}")
            continue

        items_added = 0
        for i, entry in enumerate(feed.entries[:max_items_per_feed]):
            try:
                title = getattr(entry, 'title', '').strip()
                if not title or len(title) < 10:
                    continue

                # –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                russian_finance_keywords = [
                    '–∞–∫—Ü–∏', '–±–∏—Ä–∂', '—Ä—É–±–ª', '–¥–æ–ª–ª–∞—Ä', '–Ω–µ—Ñ—Ç', '–≥–∞–∑', '—Ü–µ–Ω', '—Ä—ã–Ω–æ–∫',
                    '—ç–∫–æ–Ω–æ–º', '—Ñ–∏–Ω–∞–Ω—Å', '–±–∞–Ω–∫', '–∏–Ω–≤–µ—Å—Ç', '—Ç–æ—Ä–≥', '–ø—Ä–æ–¥–∞–∂', '–ø–æ–∫—É–ø',
                    '—Å–±–µ—Ä', '–≤—Ç–±', '–≥–∞–∑–ø—Ä–æ–º', '–ª—É–∫–æ–π–ª', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '–Ω–æ—Ä–∏–ª—å—Å–∫–∏–π',
                    '–º—Ç—Å', '—Ä–æ—Å—Ç', '–ø–∞–¥–µ–Ω–∏–µ', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–∫–≤–∞—Ä—Ç–∞–ª', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å',
                    '–º–æ—Å–±–∏—Ä–∂–∞', '–¥–æ—Ö–æ–¥', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫–∞', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è',
                    '–∫–æ–º–ø–∞–Ω–∏', '–±–∏–∑–Ω–µ—Å', '–ø—Ä–µ–¥–ø—Ä–∏—è—Ç', '–∫–æ—Ä–ø–æ—Ä–∞—Ü'
                ]

                title_lower = title.lower()
                if not any(keyword in title_lower for keyword in russian_finance_keywords):
                    continue

                rows.append({
                    'source': name,
                    'title': title,
                    'link': getattr(entry, 'link', ''),
                    'published': getattr(entry, 'published', getattr(entry, 'updated', '')),
                    'summary': getattr(entry, 'summary', '')[:200] if hasattr(entry, 'summary') else '',
                    'index': len(rows)  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
                })

                items_added += 1

            except Exception as e:
                continue

        if items_added > 0:
            print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {items_added} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {name}")

    if not rows:
        print("–ù–µ —Å–æ–±—Ä–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        # –°–æ–∑–¥–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_titles = [
            "–ê–∫—Ü–∏–∏ –ì–∞–∑–ø—Ä–æ–º–∞ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 3% –ø–æ—Å–ª–µ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö",
            "–°–±–µ—Ä–±–∞–Ω–∫ —Å–æ–æ–±—â–∞–µ—Ç –æ —Ä–µ–∫–æ—Ä–¥–Ω–æ–π –∫–≤–∞—Ä—Ç–∞–ª—å–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏ –≤ 120 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π",
            "–õ—É–∫–æ–π–ª —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –¥–∏–≤–∏–¥–µ–Ω–¥—ã –Ω–∞ 15% –ø–æ –∏—Ç–æ–≥–∞–º –≥–æ–¥–∞",
            "–†–æ—Å—Å–∏–π—Å–∫–∏–π —Ñ–æ–Ω–¥–æ–≤—ã–π —Ä—ã–Ω–æ–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–æ—Å—Ç –Ω–∞ —Ñ–æ–Ω–µ —É–∫—Ä–µ–ø–ª–µ–Ω–∏—è —Ä—É–±–ª—è",
            "–ù–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å –æ–±—ä—è–≤–ª—è–µ—Ç –æ –ø–ª–∞–Ω–∞—Ö –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞",
            "–ú–¢–° –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–µ —Ç–∞—Ä–∏—Ñ—ã –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤",
            "–†–æ—Å–Ω–µ—Ñ—Ç—å –∑–∞–∫–ª—é—á–∞–µ—Ç —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –æ –ø–æ—Å—Ç–∞–≤–∫–∞—Ö –Ω–µ—Ñ—Ç–∏ –≤ –ö–∏—Ç–∞–π",
            "–í–¢–ë —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ —á–∏—Å—Ç–æ–π –ø—Ä–∏–±—ã–ª–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≥–æ–¥",
            "–Ø–Ω–¥–µ–∫—Å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–æ—Å—Ç –≤—ã—Ä—É—á–∫–∏ –æ—Ç —Ä–µ–∫–ª–∞–º—ã –Ω–∞ 25%",
            "–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ–ª—É—á–∞—é—Ç –Ω–æ–≤—ã–µ –ª–∏—Ü–µ–Ω–∑–∏–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ",
            "–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ —Ñ–∏–∫—Å–∏—Ä—É—é—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–∏–±—ã–ª–∏ –∏–∑-–∑–∞ –ø–∞–¥–µ–Ω–∏—è —Ü–µ–Ω",
            "–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –æ–∂–∏–¥–∞—é—Ç —Ä–æ—Å—Ç–∞ –≥—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑–æ–∫ –≤ —Å–ª–µ–¥—É—é—â–µ–º –∫–≤–∞—Ä—Ç–∞–ª–µ",
            "–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ–ª—É—á–∞—é—Ç –≥–æ—Å–ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–ª—è –Ω–æ–≤—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤",
            "–†–æ–∑–Ω–∏—á–Ω—ã–µ —Å–µ—Ç–∏ —Å–æ–æ–±—â–∞—é—Ç –æ —Ä–æ—Å—Ç–µ –ø—Ä–æ–¥–∞–∂ –Ω–∞ 10%",
            "–≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –º–æ–¥–µ—Ä–Ω–∏–∑–∏—Ä—É—é—Ç —Å–µ—Ç–∏"
        ]

        for i, title in enumerate(test_titles):
            rows.append({
                'source': '—Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ',
                'title': title,
                'link': '',
                'published': datetime.now() - timedelta(days=i % 10),  # –†–∞–∑–Ω—ã–µ –¥–∞—Ç—ã
                'summary': '',
                'index': i
            })

    df = pd.DataFrame(rows)

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
    initial_count = len(df)
    df = df.drop_duplicates(subset=['title']).reset_index(drop=True)
    duplicates_removed = initial_count - len(df)

    if duplicates_removed > 0:
        print(f"–£–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π")

    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
    df['index'] = df.index

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç—ã
    df['published'] = pd.to_datetime(df['published'], errors='coerce')
    df = df.dropna(subset=['published'])

    print(f"–ò—Ç–æ–≥–æ–≤—ã–π –Ω–∞–±–æ—Ä: {len(df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    return df


def get_russian_stock_prices_safe(tickers, days=30):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–Ω —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∞–∫—Ü–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∞–∫—Ü–∏—è—Ö...")

    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    real_data = RussianStockData.get_moex_data_safe(tickers[:8], days)

    if real_data:
        print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ DataFrame
        prices_df = RussianStockData.create_dataframe_from_series(real_data, min_length=5)

        if not prices_df.empty:
            print(f"–£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω DataFrame —Å {len(prices_df)} —Å—Ç—Ä–æ–∫–∞–º–∏ –∏ {len(prices_df.columns)} –∫–æ–ª–æ–Ω–∫–∞–º–∏")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            nan_count = prices_df.isna().sum().sum()
            total_cells = prices_df.size
            nan_percentage = (nan_count / total_cells) * 100

            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {nan_count}/{total_cells} ({nan_percentage:.1f}%)")

            # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if nan_count > 0:
                prices_df = prices_df.ffill().bfill()  # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –≤–ø–µ—Ä–µ–¥ –∏ –Ω–∞–∑–∞–¥
                print("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–ø–æ–ª–Ω–µ–Ω—ã")

            return prices_df

    # –ï—Å–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ
    print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    return RussianStockData.create_realistic_test_prices(tickers, days)


# ---------------------------
# 5) –ö–õ–ê–°–° –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê –ù–û–í–û–°–¢–ï–ô –ò –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø
# ---------------------------

class NewsAnalyzer:
    def __init__(self):
        self.sector_keywords = self._build_sector_keywords()
        self.sentiment_words = self._build_sentiment_lexicon()

    def _build_sector_keywords(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Å–µ–∫—Ç–æ—Ä–æ–≤"""
        return {
            '–≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞': ['–Ω–µ—Ñ—Ç—å', '–≥–∞–∑', '–Ω–µ—Ñ—Ç–µ–≥–∞–∑', '—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫', '–Ω–µ—Ñ—Ç—è–Ω–æ–π', '–≥–∞–∑–æ–≤—ã–π', '–Ω–µ—Ñ—Ç–µ–¥–æ–±—ã—á–∞',
                           '–º–µ—Å—Ç–æ—Ä–æ–∂–¥–µ–Ω–∏–µ', '—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥', '–Ω–µ—Ñ—Ç–µ–ø—Ä–æ–¥—É–∫—Ç', '—Å–∫–≤–∞–∂–∏–Ω–∞', '–Ω–µ—Ñ—Ç–µ–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞',
                           '–≥–æ–ª—É–±–æ–µ —Ç–æ–ø–ª–∏–≤–æ'],
            '–§–∏–Ω–∞–Ω—Å—ã': ['–±–∞–Ω–∫', '—Ñ–∏–Ω–∞–Ω—Å', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫', '–≤–∫–ª–∞–¥', '–∞–∫—Ç–∏–≤', '–ø–∞—Å—Å–∏–≤', '–∫–∞–ø–∏—Ç–∞–ª',
                        '–¥–∏–≤–∏–¥–µ–Ω–¥', '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–±–∞–ª–∞–Ω—Å', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å', '–∫–≤–∞—Ä—Ç–∞–ª', '–∞–∫—Ç–∏–≤', '–ª–∏–∑–∏–Ω–≥'],
            '–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è': ['–º–µ—Ç–∞–ª–ª', '—Å—Ç–∞–ª—å', '–Ω–∏–∫–µ–ª—å', '–∞–ª—é–º–∏–Ω', '–º–µ–¥—å', '—Ü–∏–Ω–∫', '—Ä—É–¥', '–≥–æ—Ä–Ω–æ–¥–æ–±—ã–≤–∞—é—â',
                            '–ø–ª–∞—Ç–∏–Ω', '–∑–æ–ª–æ—Ç', '—Å–µ—Ä–µ–±—Ä', '–º–µ—Ç–∞–ª–ª—É—Ä–≥', '–ø—Ä–æ–∫–∞—Ç', '—á—É–≥—É–Ω', '—Ñ–µ—Ä—Ä–æ—Å–ø–ª–∞–≤'],
            '–¢–µ–ª–µ–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏': ['—Å–≤—è–∑—å', '—Ç–µ–ª–µ–∫–æ–º', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–º–æ–±–∏–ª—å–Ω', '—Ç–∞—Ä–∏—Ñ', '–∞–±–æ–Ω–µ–Ω—Ç', '—Å–µ—Ç—å',
                                 '—Ä–æ—É–º–∏–Ω–≥', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–∫–æ–º–º—É–Ω–∏–∫–∞—Ü', '–æ–ø–µ—Ä–∞—Ç–æ—Ä', '—Å–æ—Ç–æ–≤–∞—è', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø—Ä–æ–≤–∞–π–¥–µ—Ä'],
            '–•–∏–º–∏—è': ['—Ö–∏–º–∏—è', '—É–¥–æ–±—Ä–µ–Ω', '–Ω–µ—Ñ—Ç–µ—Ö–∏–º', '–ø–æ–ª–∏–º–µ—Ä', '–ø–ª–∞—Å—Ç–∏–∫', '—Ä–µ–∑–∏–Ω', '–ª–∞–∫', '–∫—Ä–∞—Å–∫',
                      '—Ö–∏–º–∏—á–µ—Å–∫', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤', '–∑–∞–≤–æ–¥', '–ø–æ–ª–∏—ç—Ç–∏–ª–µ–Ω', '–∞–º–º–∏–∞–∫'],
            '–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç': ['–∞–≤–∏–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–ø–æ—Ä—Ç', '–∞—ç—Ä–æ–ø–æ—Ä—Ç', '–≥—Ä—É–∑', '–ª–æ–≥–∏—Å—Ç–∏–∫', '–ø–µ—Ä–µ–≤–æ–∑–∫',
                          '—Å—É–¥–æ—Ö–æ–¥—Å—Ç–≤', '–∞–≤–∏–∞–ø–µ—Ä–µ–≤–æ–∑–∫', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω', '–∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂', '—Ñ—Ä–∞—Ö—Ç'],
            '–°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ': ['—Å—Ç—Ä–æ–∏—Ç–µ–ª—å', '–¥–µ–≤–µ–ª–æ–ø–µ—Ä', '–Ω–µ–¥–≤–∏–∂', '–∂–∏–ª—å–µ', '–∫–≤–∞—Ä—Ç–∏—Ä', '–¥–æ–º', '—Å—Ç—Ä–æ–∏—Ç',
                              '–∑–∞—Å—Ç—Ä–æ–π—â', '–ø—Ä–æ–µ–∫—Ç', '–æ–±—ä–µ–∫—Ç', '–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä', '–∂–∏–ª–∏—â–Ω', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å'],
            'IT-—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': ['–∏—Ç', '—Ç–µ—Ö–Ω–æ–ª–æ–≥', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–æ—Ñ—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º', '–ø—Ä–∏–ª–æ–∂–µ–Ω', '—Ü–∏—Ñ—Ä–æ–≤',
                              '–æ–Ω–ª–∞–π–Ω', '–ø–ª–∞—Ç—Ñ–æ—Ä–º', '—Å—Ç–∞—Ä—Ç–∞–ø', '–∏–Ω–Ω–æ–≤–∞—Ü', '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω', 'ai', 'ml', '–∫–∏–±–µ—Ä',
                              'digital'],
            '–†–æ–∑–Ω–∏—á–Ω–∞—è —Ç–æ—Ä–≥–æ–≤–ª—è': ['—Ä–∏—Ç–µ–π–ª', '–º–∞–≥–∞–∑–∏–Ω', '—Ç–æ—Ä–≥–æ–≤–ª—è', '—Ä–æ–∑–Ω–∏—á–Ω', '–ø–æ–∫—É–ø', '–ø—Ä–æ–¥–∞–∂',
                                   '—Ç–æ–≤–∞—Ä', '–∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç', '—Å–µ—Ç—å –º–∞–≥–∞–∑–∏–Ω', '—Ç–æ—Ä–≥–æ–≤', '—Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç', '–≥–∏–ø–µ—Ä–º–∞—Ä–∫–µ—Ç'],
            '–§–∞—Ä–º–∞—Ü–µ–≤—Ç–∏–∫–∞': ['—Ñ–∞—Ä–º–∞', '–º–µ–¥–∏—Ü–∏–Ω', '–ª–µ–∫–∞—Ä—Å—Ç–≤', '–ø—Ä–µ–ø–∞—Ä–∞—Ç', '–≤–∏—Ç–∞–º–∏–Ω', '–∑–¥–æ—Ä–æ–≤—å–µ',
                             '–±–æ–ª—å–Ω–∏', '–∫–ª–∏–Ω–∏–∫', '–∞–ø—Ç–µ–∫', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫', '–±–∏–æ—Ç–µ—Ö', '—Ñ–∞—Ä–º–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ'],
            '–≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞': ['—ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥', '—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç', '—ç–ª–µ–∫—Ç—Ä–æ—Å–µ—Ç', '—ç–Ω–µ—Ä–≥–æ—Å–∏—Å—Ç–µ–º',
                                  '—ç–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–Ω—Ü', '–∞—Ç–æ–º–Ω', '—Ç–µ–ø–ª–æ—ç–Ω–µ—Ä–≥', '–≥–∏–¥—Ä–æ—ç–Ω–µ—Ä–≥', '—ç–Ω–µ—Ä–≥–æ–±–ª–æ–∫', '–ø–æ–¥—Å—Ç–∞–Ω—Ü']
        }

    def _build_sentiment_lexicon(self):
        """–°–ª–æ–≤–∞—Ä—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        return {
            'positive': [
                '—Ä–æ—Å—Ç', '–≤—ã—Ä–æ—Å', '—É–≤–µ–ª–∏—á–∏–ª', '—É–≤–µ–ª–∏—á–µ–Ω–∏–µ', '–ø—Ä–∏–±—ã–ª—å', '–¥–æ—Ö–æ–¥', '—É—Å–ø–µ—Ö', '—Ä–µ–∫–æ—Ä–¥',
                '—É–ª—É—á—à–µ–Ω–∏–µ', '–ø–æ–∑–∏—Ç–∏–≤', '—Å–∏–ª—å–Ω—ã–π', '—Å—Ç–∞–±–∏–ª—å–Ω', '–ª–∏–¥–µ—Ä', '–∏–Ω–Ω–æ–≤–∞—Ü', '–ø—Ä–æ—Ä—ã–≤',
                '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω', '–ø—Ä–æ—Ü–≤–µ—Ç–∞–Ω', '—É–≤–µ—Ä–µ–Ω', '–æ–ø—Ç–∏–º–∏–∑–º', '–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–ø—Ä–µ–≤—ã—à–µ–Ω',
                '—É—Å–ø–µ—à–Ω', '–≤—ã—Å–æ–∫', '–ø—Ä–∏—Ä–æ—Å—Ç', '—Ä–∞—Å—à–∏—Ä–µ–Ω', '—Ä–∞–∑–≤–∏—Ç', '–º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü', '–∏–Ω–≤–µ—Å—Ç–∏—Ü'
            ],
            'negative': [
                '–ø–∞–¥–µ–Ω–∏–µ', '—Å–Ω–∏–∂–µ–Ω', '—É–ø–∞–ª', '—Å–æ–∫—Ä–∞—â–µ–Ω', '—É–±—ã—Ç–æ–∫', '–ø—Ä–æ–±–ª–µ–º', '–∫—Ä–∏–∑–∏—Å', '—Å–ª–∞–±',
                '–Ω–µ–≥–∞—Ç–∏–≤', '—Ä–∏—Å–∫', '–æ–ø–∞—Å–µ–Ω', '–ø–æ—Ç–µ—Ä—è', '–æ–±–≤–∞–ª', '–¥–µ—Ñ–∏—Ü–∏—Ç', '–±–∞–Ω–∫—Ä–æ—Ç', '–¥–æ–ª–≥',
                '—Å–ª–æ–∂–Ω', '–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω', '—É—Ö—É–¥—à–µ–Ω', '—Å–ø–∞–¥', '–∑–∞–º–µ–¥–ª–µ–Ω', '—Å–Ω–∏–∂–∞—Ç—å—Å—è', '—Å–ª–æ–∂–Ω–æ—Å—Ç',
                '—Å–æ–∫—Ä–∞—â–µ–Ω', '—É–≤–æ–ª—å–Ω–µ–Ω', '–∑–∞–∫—Ä—ã—Ç', '–ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤', '–¥–µ–≤–∞–ª—å–≤–∞—Ü', '–∏–Ω—Ñ–ª—è—Ü'
            ],
            'intensifiers': [
                '–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω', '—Ä–µ–∑–∫', '—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω', '–∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω', '—Ä–∞–¥–∏–∫–∞–ª—å–Ω', '–º–∞—Å—à—Ç–∞–±–Ω',
                '–∫—Ä—É–ø–Ω', '—Å–µ—Ä—å–µ–∑–Ω', '–∫—Ä–∞–π–Ω', '–æ—á–µ–Ω—å', '—Å–∏–ª—å–Ω', '–∫—Ä–∞–π–Ω–µ', '–º–∞–∫—Å–∏–º–∞–ª—å–Ω', '—Ä–µ–∫–æ—Ä–¥–Ω'
            ]
        }

    def analyze_news_sentiment(self, title):
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        title_lower = title.lower()

        positive_score = 0
        negative_score = 0

        # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        for word in self.sentiment_words['positive']:
            if word in title_lower:
                positive_score += 1
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Å–∏–ª–∏—Ç–µ–ª–∏
                for intensifier in self.sentiment_words['intensifiers']:
                    if f"{intensifier} {word}" in title_lower or f"{word} {intensifier}" in title_lower:
                        positive_score += 0.5
                        break

        # –ê–Ω–∞–ª–∏–∑ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        for word in self.sentiment_words['negative']:
            if word in title_lower:
                negative_score += 1
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Å–∏–ª–∏—Ç–µ–ª–∏
                for intensifier in self.sentiment_words['intensifiers']:
                    if f"{intensifier} {word}" in title_lower or f"{word} {intensifier}" in title_lower:
                        negative_score += 0.5
                        break

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        total_score = positive_score - negative_score
        if total_score > 0:
            return min(1.0, total_score * 0.15)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        elif total_score < 0:
            return max(-1.0, total_score * 0.15)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        else:
            return 0.0

    def predict_sector_movement(self, news_df):
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è —Å–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        sector_scores = {sector: 0.0 for sector in RUSSIAN_SECTORS.keys()}
        sector_news_count = {sector: 0 for sector in RUSSIAN_SECTORS.keys()}

        for _, news in news_df.iterrows():
            title = news['title']
            sentiment = self.analyze_news_sentiment(title)

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏
            affected_sectors = self._identify_sectors_from_title(title)

            for sector in affected_sectors:
                sector_scores[sector] += sentiment
                sector_news_count[sector] += 1

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç -5 –¥–æ +5
        recommendations = {}
        for sector in RUSSIAN_SECTORS.keys():
            if sector_news_count[sector] > 0:
                avg_score = sector_scores[sector] / sector_news_count[sector]
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —à–∫–∞–ª—É -5 –¥–æ +5
                recommendation = int(round(avg_score * 5))
                recommendation = max(-5, min(5, recommendation))  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
            else:
                recommendation = 0  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π

            recommendations[sector] = recommendation

        return recommendations

    def _identify_sectors_from_title(self, title):
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        title_lower = title.lower()
        detected_sectors = set()

        for sector, keywords in self.sector_keywords.items():
            for keyword in keywords:
                if keyword in title_lower:
                    detected_sectors.add(sector)
                    break

        return list(detected_sectors) if detected_sectors else ['–§–∏–Ω–∞–Ω—Å—ã']  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –§–∏–Ω–∞–Ω—Å—ã


# ---------------------------
# 6) –ö–õ–ê–°–° –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ò–ò
# ---------------------------

class StockAIModel:
    def __init__(self, model_path='stock_ai_model.joblib'):
        self.model_path = model_path
        self.model = None
        self.is_trained = False

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if os.path.exists(self.model_path):
            try:
                self.model = joblib.load(self.model_path)
                self.is_trained = True
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –ò–ò –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {self.model_path}")
                return True
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

    def create_new_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        self.model = RandomForestRegressor(
            n_estimators=50,  # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            max_depth=8,
            random_state=42,
            n_jobs=-1
        )
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å Random Forest")

    def generate_training_data(self, news_df, prices_df):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ"""
        print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

        analyzer = NewsAnalyzer()

        # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à–µ –¥–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        dates = pd.date_range(end=datetime.now(), periods=60, freq='D')

        X = []
        y = []

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for i in range(len(dates) - 5):
            # –°–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å–∏–º—É–ª—è—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π)
            features = []
            for sector in RUSSIAN_SECTORS.keys():
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞
                sentiment = np.random.normal(0, 0.3)  # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∫—Ä—É–≥ 0
                sentiment = max(-1, min(1, sentiment))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
                features.append(sentiment)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            # –ë–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–π —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç -> –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
            base_return = np.sum(features) * 0.1  # –ë–∞–∑–æ–≤–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞
            noise = np.random.normal(0, 0.02)  # –®—É–º
            target_return = base_return + noise

            X.append(features)
            y.append(target_return)

        return np.array(X), np.array(y)

    def prepare_training_data(self, news_df, prices_df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ò–ò...")

            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ
            if len(news_df) < 5 or len(prices_df) < 10:
                print("‚ö†Ô∏è  –†–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ...")
                return self.generate_training_data(news_df, prices_df)

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
            analyzer = NewsAnalyzer()

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–µ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
            news_df['date'] = pd.to_datetime(news_df['published']).dt.date
            daily_sentiments = []

            for date in news_df['date'].unique():
                daily_news = news_df[news_df['date'] == date]
                sector_scores = analyzer.predict_sector_movement(daily_news)
                sector_scores['date'] = date
                daily_sentiments.append(sector_scores)

            if not daily_sentiments:
                print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")
                return self.generate_training_data(news_df, prices_df)

            sentiment_df = pd.DataFrame(daily_sentiments)
            sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∞–∫—Ü–∏–π
            returns_df = prices_df.pct_change().dropna()

            if returns_df.empty:
                print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")
                return self.generate_training_data(news_df, prices_df)

            # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (—Å—Ä–µ–¥–Ω—è—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –ø–æ —Å–µ–∫—Ç–æ—Ä–∞–º)
            targets = {}
            for sector, tickers in RUSSIAN_SECTORS.items():
                sector_tickers = [t for t in tickers if t in returns_df.columns]
                if sector_tickers:
                    targets[sector] = returns_df[sector_tickers].mean(axis=1)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–∏
            X = []
            y = []

            valid_pairs = 0
            for i in range(1, len(returns_df)):
                current_date = returns_df.index[i]
                prev_date = returns_df.index[i - 1]

                # –ü—Ä–∏–∑–Ω–∞–∫–∏: sentiment –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π –¥–µ–Ω—å
                prev_sentiment = sentiment_df[sentiment_df['date'] == prev_date]
                if not prev_sentiment.empty:
                    features = []
                    for sector in RUSSIAN_SECTORS.keys():
                        feature_value = prev_sentiment[sector].iloc[0] if sector in prev_sentiment.columns else 0
                        features.append(feature_value)

                    # –¶–µ–ª—å: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å
                    current_returns = []
                    for sector in RUSSIAN_SECTORS.keys():
                        if sector in targets and current_date in targets[sector].index:
                            current_returns.append(targets[sector].loc[current_date])
                        else:
                            current_returns.append(0)

                    if current_returns and not all(r == 0 for r in current_returns):
                        X.append(features)
                        y.append(np.mean(current_returns))
                        valid_pairs += 1

            print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {valid_pairs} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")

            if valid_pairs < 10:
                print("‚ö†Ô∏è  –ú–∞–ª–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –¥–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ...")
                X_synth, y_synth = self.generate_training_data(news_df, prices_df)
                X = np.vstack([X, X_synth]) if len(X) > 0 else X_synth
                y = np.concatenate([y, y_synth]) if len(y) > 0 else y_synth

            return np.array(X), np.array(y)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ...")
            return self.generate_training_data(news_df, prices_df)

    def train(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if len(X) == 0 or len(y) == 0:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False

        try:
            print(f"üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ {len(X)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

            if not self.is_trained:
                # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                self.model.fit(X, y)
                self.is_trained = True
                print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞")
            else:
                # –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
                self.model.fit(X, y)
                print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–∞")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            joblib.dump(self.model, self.model_path)
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.model_path}")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False

    def predict(self, X):
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏"""
        if self.model is None or not self.is_trained:
            return np.zeros(len(X))

        try:
            return self.model.predict(X)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return np.zeros(len(X))


# ---------------------------
# 7) –û–°–ù–û–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –ü–†–û–ì–†–ê–ú–ú–´
# ---------------------------

def save_recommendations(recommendations, filename='RECOM.csv'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ CSV —Ñ–∞–π–ª"""
    df = pd.DataFrame(list(recommendations.items()), columns=['–°–µ–∫—Ç–æ—Ä', '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è'])
    df['–û–ø–∏—Å–∞–Ω–∏–µ'] = df['–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è'].map(lambda x:
                                            '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–æ–∫—É–ø–∫–∞' if x == 5 else
                                            '–°–∏–ª—å–Ω–∞—è –ø–æ–∫—É–ø–∫–∞' if x >= 3 else
                                            '–£–º–µ—Ä–µ–Ω–Ω–∞—è –ø–æ–∫—É–ø–∫–∞' if x >= 1 else
                                            '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ' if x == 0 else
                                            '–£–º–µ—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞' if x >= -2 else
                                            '–°–∏–ª—å–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞' if x >= -4 else
                                            '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞'
                                            )
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"üíæ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")


def main():
    print("=== –°–ò–°–¢–ï–ú–ê –ê–ù–ê–õ–ò–ó–ê –†–û–°–°–ò–ô–°–ö–ò–• –ê–ö–¶–ò–ô –° –ò–ò ===")
    print("–í–µ—Ä—Å–∏—è —Å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å–µ–∫—Ç–æ—Ä–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏–µ–º –ò–ò\n")

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò –º–æ–¥–µ–ª–∏
        ai_model = StockAIModel()

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if not ai_model.load_model():
            ai_model.create_new_model()

        # 1. –°–±–æ—Ä —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        print("1. üì∞ –°–±–æ—Ä —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
        news_df = fetch_russian_feeds_safe(RUSSIAN_FEEDS, max_items_per_feed=15)

        if news_df.empty:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            return

        print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(news_df)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–± –∞–∫—Ü–∏—è—Ö
        print("\n2. üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∞–∫—Ü–∏—è—Ö...")
        all_tickers = []
        for sector_tickers in RUSSIAN_SECTORS.values():
            all_tickers.extend(sector_tickers)

        prices_data = get_russian_stock_prices_safe(all_tickers, days=45)  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ø–µ—Ä–∏–æ–¥

        if prices_data.empty:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ–± –∞–∫—Ü–∏—è—Ö. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
            return

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(prices_data.columns)} —Ç–∏–∫–µ—Ä–æ–≤ –∑–∞ {len(prices_data)} –¥–Ω–µ–π")

        # 3. –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–∫—Ç–æ—Ä–æ–≤
        print("\n3. üîç –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–∫—Ç–æ—Ä–æ–≤...")
        analyzer = NewsAnalyzer()
        recommendations = analyzer.predict_sector_movement(news_df)

        # 4. –û–±—É—á–µ–Ω–∏–µ –ò–ò –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("\n4. üß† –û–±—É—á–µ–Ω–∏–µ –ò–ò –º–æ–¥–µ–ª–∏...")
        X, y = ai_model.prepare_training_data(news_df, prices_data)

        if ai_model.train(X, y):
            print("üéØ –û–±—É—á–µ–Ω–∏–µ –ò–ò –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        else:
            print("‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ò–ò –Ω–µ —É–¥–∞–ª–æ—Å—å, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")

        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        print("\n5. üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
        save_recommendations(recommendations)

        # 6. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n6. üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        print("=" * 60)
        for sector, recommendation in sorted(recommendations.items(), key=lambda x: x[1], reverse=True):
            emoji = "üü¢" if recommendation > 0 else "üî¥" if recommendation < 0 else "‚ö™"
            action = "–ü–û–ö–£–ü–ö–ê" if recommendation > 0 else "–ü–†–û–î–ê–ñ–ê" if recommendation < 0 else "–ù–ï–ô–¢–†–ê–õ–¨–ù–û"
            print(f"{emoji} {sector:25} {recommendation:+2d} ({action})")

        print("=" * 60)
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ RECOM.csv")

        # 7. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        positive_recs = sum(1 for r in recommendations.values() if r > 0)
        negative_recs = sum(1 for r in recommendations.values() if r < 0)
        neutral_recs = len(recommendations) - positive_recs - negative_recs

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
        print(f"   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –ø–æ–∫—É–ø–∫—É: {positive_recs}")
        print(f"   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ–¥–∞–∂—É: {negative_recs}")
        print(f"   ‚Ä¢ –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {neutral_recs}")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–µ–∫—Ç–æ—Ä–æ–≤: {len(recommendations)}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        summary_report = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'news_count': len(news_df),
            'stocks_count': len(prices_data.columns),
            'period_days': len(prices_data),
            'positive_recommendations': positive_recs,
            'negative_recommendations': negative_recs,
            'neutral_recommendations': neutral_recs,
            'ai_model_trained': ai_model.is_trained,
            'status': 'success'
        }

        import json
        with open('analysis_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, indent=2, ensure_ascii=False)

        print(f"\nüíæ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: analysis_summary.json")

    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main(): {e}")
        import traceback
        traceback.print_exc()


# ---------------------------
# 8) –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
# ---------------------------

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∞–∫—Ü–∏–π —Å –ò–ò...")
    start_time = time.time()

    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\nüí• –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()
    finally:
        execution_time = time.time() - start_time
        print(f"\n‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f} —Å–µ–∫—É–Ω–¥")
        print("üèÅ –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")